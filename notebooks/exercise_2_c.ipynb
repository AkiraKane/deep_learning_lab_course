{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/Users/riestern/Development/Repositories/deep_learning_lab_course\")\n",
    "\n",
    "import numpy as np\n",
    "import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CNNs on MNIST\n",
    "In the third part of the exercise we will now apply CNNs to MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, take a look at the neural network code I placed into the nn package in this repository. It should look familiar as it is mainly the code you used in the last exercise. One thing that I added is a prototyped implementation of convolution and pooling. You will find these in nn/conv/layers.py.\n",
    "\n",
    "After you have completed exercises 2 a) and 2 b) you should go into that file, and implement the missing pieces, which will essentially be the conv and pool functions you have already written as well as their backward pass (which might be a bit more tricky). \n",
    "\n",
    "Once you implemented those, come back here and make sure the following example works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "First, let us do gradient checking using your conv and pooling layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_shape = (5, 1, 28, 28)\n",
    "n_labels = 6\n",
    "layers = [nn.InputLayer(input_shape)]\n",
    "\n",
    "layers.append(nn.Conv(\n",
    "                layers[-1],\n",
    "                n_feats=2,\n",
    "                filter_shape=(3,3),\n",
    "                init_stddev=0.01,\n",
    "                activation_fun=nn.Activation('relu'),\n",
    "))\n",
    "layers.append(nn.Pool(layers[-1]))\n",
    "layers.append(nn.Flatten(layers[-1]))\n",
    "layers.append(nn.FullyConnectedLayer(\n",
    "                layers[-1],\n",
    "                num_units=6,\n",
    "                init_stddev=0.1,\n",
    "                activation_fun=None\n",
    "))\n",
    "layers.append(nn.SoftmaxOutput(layers[-1]))\n",
    "net = nn.NeuralNetwork(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create random data\n",
    "X = np.random.normal(size=input_shape)\n",
    "Y = np.zeros((input_shape[0], n_labels))\n",
    "for i in range(Y.shape[0]):\n",
    "    idx = np.random.randint(n_labels)\n",
    "    Y[i, idx] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking gradient for layer 1\n",
      "diff 8.93e-01\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-818bfe7bc778>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# perform gradient checking, this should go through if you implemented everything correctly!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/k-gee/devel/deep_learning_lab_course/nn/network.pyc\u001b[0m in \u001b[0;36mcheck_gradients\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m    151\u001b[0m                     \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgparam_bprop\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mgparam_fd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m                     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'diff {:.2e}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m                     \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m                     \u001b[1;31m# reset the parameters to their initial values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# perform gradient checking, this should go through if you implemented everything correctly!\n",
    "net.check_gradients(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on mnist\n",
    "Finally, figure out a reasonable network architecture and train it on MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... downloading MNIST from the web\n",
      "... loading data\n",
      "... done loading data\n"
     ]
    }
   ],
   "source": [
    "# you can load the mnist data as \n",
    "data = nn.data.mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (1, 3, 5, 5)\n",
      "Weight shape: (3, 2, 3, 3)\n",
      "[[[[0 0 0 0 0 0 0]\n",
      "   [0 2 2 2 2 1 0]\n",
      "   [0 1 1 1 1 0 0]\n",
      "   [0 0 1 0 1 2 0]\n",
      "   [0 2 2 0 2 0 0]\n",
      "   [0 0 2 1 0 1 0]\n",
      "   [0 0 0 0 0 0 0]]\n",
      "\n",
      "  [[0 0 0 0 0 0 0]\n",
      "   [0 0 0 1 0 0 0]\n",
      "   [0 0 1 2 1 1 0]\n",
      "   [0 0 0 0 2 1 0]\n",
      "   [0 2 2 0 2 0 0]\n",
      "   [0 1 1 0 0 0 0]\n",
      "   [0 0 0 0 0 0 0]]\n",
      "\n",
      "  [[0 0 0 0 0 0 0]\n",
      "   [0 0 1 0 0 0 0]\n",
      "   [0 2 0 1 1 2 0]\n",
      "   [0 0 0 0 0 0 0]\n",
      "   [0 2 2 1 1 1 0]\n",
      "   [0 1 2 1 0 1 0]\n",
      "   [0 0 0 0 0 0 0]]]]\n",
      "Output shape: (1, 2, 5, 5)\n",
      "Output: \n",
      "[[[[  0.  -2.  -3.  -5.   2.]\n",
      "   [  1.   1.  -2.  -2.  -1.]\n",
      "   [ -1.   4.   4.  -4.  -1.]\n",
      "   [ -2.  -3.   4.  -1.  -2.]\n",
      "   [ -1.  -1.  -4.   3.  -1.]]\n",
      "\n",
      "  [[ -3.  -8.  -9.  -9.  -5.]\n",
      "   [ -1.  -4.  -9. -10.  -7.]\n",
      "   [ -4.  -9. -11.  -3.  -9.]\n",
      "   [ -2.  -8.  -6.  -6.  -6.]\n",
      "   [  5.  -4.  -1.  -2.  -1.]]]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/Users/riestern/Development/Repositories/deep_learning_lab_course\")\n",
    "import numpy as np\n",
    "import nn\n",
    "\n",
    "input_shape = (1, 3, 5, 5)\n",
    "layers = [nn.InputLayer(input_shape)]\n",
    "conv_layer = nn.Conv(\n",
    "                layers[-1],\n",
    "                n_feats=2,\n",
    "                filter_shape=(3,3),\n",
    "                init_stddev=0.01,\n",
    "                activation_fun=None)\n",
    "layers.append(conv_layer)\n",
    "layers.append(nn.LinearOutput(layers[-1]))\n",
    "\n",
    "example = np.array([[[[2,2,2,2,1],[1,1,1,1,0],[0,1,0,1,2],[2,2,0,2,0],[0,2,1,0,1]],[[0,0,1,0,0],[0,1,2,1,1],[0,0,0,2,1],[2,2,0,2,0],[1,1,0,0,0]],[[0,1,0,0,0],[2,0,1,1,2],[0,0,0,0,0],[2,2,1,1,1],[1,2,1,0,1]]]])\n",
    "print(\"Input shape: \" + str(example.shape))\n",
    "\n",
    "w_0_0 = np.array([[0,1,-1],[-1,-1,0],[1,-1,1]])\n",
    "w_0_1 = np.array([[-1,0,0],[-1,-1,1],[0,0,-1]])\n",
    "w_0_2 = np.array([[1,0,-1],[1,0,0],[1,1,-1]])\n",
    "b_0 = 1\n",
    "\n",
    "w_1_0 = np.array([[-1,-1,0],[0,-1,0],[-1,0,-1]])\n",
    "w_1_1 = np.array([[0,-1,1],[-1,0,0],[-1,-1,-1]])\n",
    "w_1_2 = np.array([[0,1,1],[-1,1,1],[-1,0,0]])\n",
    "b_1 = 0\n",
    "\n",
    "conv_layer.W = np.array([[w_0_0,w_1_0],[w_0_1,w_1_1],[w_0_2,w_1_2]])\n",
    "conv_layer.b = np.array([b_0, b_1])\n",
    "print(\"Weight shape: \" + str(conv_layer.W.shape))\n",
    "\n",
    "\n",
    "net = nn.NeuralNetwork(layers)\n",
    "output = net.predict(example)\n",
    "print(\"Output shape: \" + str(output.shape))\n",
    "print(\"Output: \\n\" + str(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
